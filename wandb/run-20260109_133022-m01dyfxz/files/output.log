Resuming from checkpoint: ckpt.pt
D:\tiny-llm-model\venv\Lib\site-packages\torch\_dynamo\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
D:\tiny-llm-model\venv\Lib\site-packages\torch\utils\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "D:\tiny-llm-model\train.py", line 263, in <module>
    loss.backward()
  File "D:\tiny-llm-model\venv\Lib\site-packages\torch\_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "D:\tiny-llm-model\venv\Lib\site-packages\torch\autograd\__init__.py", line 353, in backward
    _engine_run_backward(
  File "D:\tiny-llm-model\venv\Lib\site-packages\torch\autograd\graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
