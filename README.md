# The Observatory: Personal AI Research Lab

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

**The Observatory** is a minimalist, modular, and rigorous deep learning research framework designed from first principles.

Unlike production-oriented libraries that prioritize inference latency or ease of deployment, this lab prioritizes **observability**, **ablation**, and **scientific rigor**. It is a "Glass Box" environment where every variableâ€”from initialization variance to gradient accumulationâ€”is exposed, measurable, and controllable.

---

## ðŸ”¬ Core Philosophy

1.  **Code as Controlled Environment:** The codebase is a scientific instrument. Its primary purpose is to isolate variables to measure the impact of hypotheses.
2.  **No Magic:** We avoid "black box" abstractions. Layer norms, attention masks, and optimizer groups are implemented explicitly to allow for surgical modification.
3.  **Evaluation as First-Class Citizen:** We do not just measure "loss." We measure **Model FLOPs Utilization (MFU)**, **Gradient Norms**, and **Perplexity** to understand *learning dynamics*, not just outcomes.
4.  **Immutable Experiments:** Configurations are frozen at runtime. We do not "fix" running experiments; we iterate.

---

## ðŸ“‚ Architecture

The project is structured to separate **Model Definition** (Math) from **Training Logic** (Environment).

```text
research_lab/
â”œâ”€â”€ configs/                 # The Control Room (Hyperparameters)
â”‚   â”œâ”€â”€ model/               # Architecture definitions (GPT-Nano, etc.)
â”‚   â””â”€â”€ trainer/             # Training loop settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              # Pure mathematical definitions (Stateless)
â”‚   â”‚   â”œâ”€â”€ components/      # Atomic layers (Attention, MLP, Norms)
â”‚   â”‚   â””â”€â”€ transformer.py   # The GPT Block composition
â”‚   â”œâ”€â”€ training/            # The "Trainer" abstraction
â”‚   â”‚   â””â”€â”€ trainer.py       # The step, forward, backward sequence
â”‚   â”œâ”€â”€ data/                # Data Loading & Tokenization
â”‚   â”‚   â”œâ”€â”€ prepare.py       # Tokenization script (Text -> Binary)
â”‚   â”‚   â””â”€â”€ dataset.py       # Memory-mapped dataset loader
â”‚   â””â”€â”€ evaluation/          # The Judge
â”‚       â””â”€â”€ metrics.py       # Perplexity, MFU, Generation
â”œâ”€â”€ analysis/                # Post-mortem notebooks
â”‚   â””â”€â”€ notebooks/           # Jupyter notebooks for comparative analysis
â”œâ”€â”€ experiments/             # OUTPUTS (Git-ignored)
â”‚   â””â”€â”€ [timestamp]_exp/     # Checkpoints, logs, and configs
â””â”€â”€ train.py                 # The entry point